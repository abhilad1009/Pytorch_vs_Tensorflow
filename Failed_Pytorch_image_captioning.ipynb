{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2s1A9eLRPEj"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TGZmOuqMia9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f52b65-4ce2-45a3-e18b-881b1330b12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ6q39Vd-y-7"
      },
      "source": [
        "This tutorial uses lots of imports, mostly for loading the dataset(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "# import transformers\n",
        "# import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqGXX9Dc5c0v"
      },
      "source": [
        "#### Flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaNy_l7tGuAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import collections\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "def flickr8k(path='flickr8k'):\n",
        "    path = pathlib.Path(path)\n",
        "\n",
        "    if len(list(path.rglob('*'))) < 16197:\n",
        "        download_and_extract_archive(\n",
        "            url='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "            download_root='.',\n",
        "            extract_root=path,\n",
        "        )\n",
        "        download_and_extract_archive(\n",
        "            url='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "            download_root='.',\n",
        "            extract_root=path,\n",
        "        )\n",
        "\n",
        "    captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "    captions = (line.split('\\t') for line in captions)\n",
        "    captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "    cap_dict = collections.defaultdict(list)\n",
        "    for fname, cap in captions:\n",
        "        cap_dict[fname].append(cap)\n",
        "\n",
        "    train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "    train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "    test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "    test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "    return train_captions, test_captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAagBw5p-TM"
      },
      "source": [
        "#### Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFtTZaobquNr"
      },
      "source": [
        "The Flickr8k is a good choice because it contains 5-captions per image, more data for a smaller download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJySPbzJ4Wxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13549a57-e626-4bad-88ca-a567207a513a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230508T032652Z&X-Amz-Expires=300&X-Amz-Signature=97d1b10a77513c9447d12f2ffdd901c2129ff084141e3e660fe8d99222175b4d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream to ./Flickr8k_Dataset.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1115419746/1115419746 [00:21<00:00, 52561305.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./Flickr8k_Dataset.zip to flickr8k\n",
            "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230508%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230508T032721Z&X-Amz-Expires=300&X-Amz-Signature=273560cfe9cc953a177c8d8cc1dcfb49a184f777e122fb45d1271ea84baa6098&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream to ./Flickr8k_text.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2340801/2340801 [00:00<00:00, 93828542.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./Flickr8k_text.zip to flickr8k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "choose = 'flickr8k'\n",
        "\n",
        "if choose == 'flickr8k':\n",
        "  train_raw, test_raw = flickr8k()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UAc275FHxm8"
      },
      "source": [
        "The loaders for both datasets above return `tf.data.Dataset`s containing `(image_path, captions)` pairs. The Flickr8k dataset contains 5 captions per image, while Conceptual Captions has 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIa0ZaP4tBez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbcfcd0-c7f5-4979-eadf-7d84112e965d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flickr8k/Flicker8k_Dataset/2513260012_03d33305cf.jpg\n",
            "['A black dog is running after a white dog in the snow .', 'Black dog chasing brown dog through snow', 'Two dogs chase each other across the snowy ground .', 'Two dogs play together in the snow .', 'Two dogs running through a low lying body of water .']\n"
          ]
        }
      ],
      "source": [
        "for ex_path, ex_captions in train_raw[:1]:\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Image feature extractor\n",
        "\n",
        "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlUckK8Zfikv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6f2670-88ab-439a-ada1-5037ee1ea6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 152MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "\n",
        "IMAGE_SHAPE = (224, 224, 3)\n",
        "\n",
        "mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
        "mobilenet = mobilenet.features\n",
        "mobilenet.eval()\n",
        "\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "preprocessing = Compose([Resize(IMAGE_SHAPE[:2]), ToTensor(), normalize])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dojkiou9gL3R"
      },
      "source": [
        "Here's a function to load an image and resize it for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "IMAGE_SHAPE = (224, 224, 3)\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    transform = transforms.Resize(IMAGE_SHAPE[:-1])\n",
        "    img = transform(img)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JyQ7zS6gzZh"
      },
      "source": [
        "The model returns a feature map for each image in the input batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY86n2i6wJNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35948914-3041-4d96-ed67-88cdd067f8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 576, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "ex_path = 'flickr8k/Flicker8k_Dataset/2513260012_03d33305cf.jpg'  # Replace this with the path to your image\n",
        "\n",
        "test_img = load_image(ex_path)\n",
        "test_img_batch = preprocessing(test_img).unsqueeze(0)\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Setup the text tokenizer/vectorizer\n",
        "\n",
        "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
        "\n",
        "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n",
        "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n",
        "* Create word-to-index and index-to-word mappings to display results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NroZIzB90hD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "534bae3f-f4e0-4415-be22-c520323a1864"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-97d8a0b15a65>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Learn the vocabulary from the caption data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrain_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Print the first 10 words in the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/vocab/vocab_factory.py\u001b[0m in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-97d8a0b15a65>\u001b[0m in \u001b[0;36mtokenize_iterator\u001b[0;34m(data_iter)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Use the top 5000 words for a vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-97d8a0b15a65>\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[{re.escape(string.punctuation)}]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[START]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[END]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Flickr8k\n",
        "\n",
        "# Load the Flickr8k dataset\n",
        "train_dataset, val_dataset = Flickr8k(split=('train', 'val'))\n",
        "\n",
        "# Define a tokenizer to split the captions into words\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Use a generator to iterate over all captions and tokenize them\n",
        "def tokenize_caption(dataset):\n",
        "    for caption in dataset.captions:\n",
        "        yield tokenizer(caption)\n",
        "\n",
        "# Build a vocabulary of the top words in the captions\n",
        "vocab = build_vocab_from_iterator(tokenize_caption(train_dataset), min_freq=2)\n",
        "\n",
        "# Define a TextVectorization layer to transform the captions into integer sequences\n",
        "text_vectorizer = torch.nn.Sequential(\n",
        "    torchtext.experimental.transforms.text.DefaultPreprocessor(),\n",
        "    torchtext.experimental.transforms.text.TextSequential(tokenizer),\n",
        "    torchtext.experimental.transforms.text.VocabTransform(vocab),\n",
        "    torchtext.experimental.transforms.text.NumericReplaceTransform(\n",
        "        vocabulary=vocab, unknown_token=0, special_tokens={}),\n",
        "    torchtext.experimental.transforms.text.PadTransform(50, 0))\n",
        "\n",
        "# Adapt the TextVectorization layer to the training set\n",
        "text_vectorizer.adapt([example.caption for example in train_dataset])\n",
        "\n",
        "# Create word-to-index and index-to-word mappings to display results\n",
        "vocab_list = text_vectorizer.get_vocabulary()\n",
        "word_to_index = {word: index for index, word in enumerate(vocab_list)}\n",
        "index_to_word = {index: word for index, word in enumerate(vocab_list)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9SQOXFsyS36"
      },
      "outputs": [],
      "source": [
        "# Use the top 5000 words for a vocabulary.\n",
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRahTDtWhJIf"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2mGxD33JCxN"
      },
      "outputs": [],
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = vocab.lookup_indices\n",
        "index_to_word = vocab.lookup_token\n",
        "\n",
        "# Example usage\n",
        "example_words = ['[START]', 'a', 'cat']\n",
        "example_indices = [word_to_index(word) for word in example_words]\n",
        "print(\"Words to indices:\", example_indices)\n",
        "\n",
        "example_indices = [0, 1, 2]\n",
        "example_words = [index_to_word(index) for index in example_indices]\n",
        "print(\"Indices to words:\", example_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo-cfCX3LnHs"
      },
      "outputs": [],
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrUUfGc65vAT"
      },
      "outputs": [],
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "### Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX0Z_98S2tN"
      },
      "source": [
        "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
        "\n",
        "This function will replicate the image so there are 1:1 images to captions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Lqwl9NiGT0"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZGUsuGzUfzt"
      },
      "outputs": [],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENR_-swVhnm"
      },
      "source": [
        "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DsgQ_hZT4C2"
      },
      "outputs": [],
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA1x2j0JXX-N"
      },
      "source": [
        "This function adds operations to a dataset. The steps are:\n",
        "\n",
        "1. Load the images (and ignore images that fail to load).\n",
        "2. Replicate images to match the number of captions.\n",
        "3. Shuffle and rebatch the `image, caption` pairs.\n",
        "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
        "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Pt9zldjQ0q"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrQ85t1GNfpQ"
      },
      "source": [
        "You could install the feature extractor in your model and train on the datasets like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KlhOG5cjQ0r"
      },
      "outputs": [],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Zy9F3zX7i2"
      },
      "outputs": [],
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798DtfH51UI8"
      },
      "source": [
        " </section>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI265LiDslr2"
      },
      "source": [
        "## Data ready for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jfb8qknlsKi"
      },
      "source": [
        "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJBEwuXLZQdw"
      },
      "outputs": [],
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22R58DzZoF17"
      },
      "source": [
        "The input tokens and the labels are the same, just shifted by 1 step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7h5UGftn1hT"
      },
      "outputs": [],
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfICM49WFpIb"
      },
      "source": [
        "## A Transformer decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRXWwIKNybB"
      },
      "source": [
        "The model will be implemented in three main parts: \n",
        "\n",
        "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
        "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
        "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
        "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
        "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
        "1. Output - A multiclass-classification over the output vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngm3SQMCaYU"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9suaARZGPKw"
      },
      "source": [
        "The input text has already been split up into tokens and converted to sequences of IDs. \n",
        "\n",
        "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
        "\n",
        "The `SeqEmbedding` layer defined below:\n",
        "\n",
        "- It looks up the embedding vector for each token.\n",
        "- It looks up an embedding vector for each sequence location.\n",
        "- It adds the two together.\n",
        "- It uses `mask_zero=True` to initialize the keras-masks for the model.\n",
        "\n",
        "Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P91LU2F0a9Ga"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SeqEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, depth):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(num_embeddings=max_length, embedding_dim=depth)\n",
        "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=depth, padding_idx=0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "        x = torch.arange(seq.size(1), device=seq.device)  # (seq)\n",
        "        x = x.unsqueeze(0)  # (1, seq)\n",
        "        x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "        return seq + x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II1mD-bBCdMB"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMLeMtKPTCW"
      },
      "source": [
        "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
        "\n",
        "The `CausalSelfAttention` layer is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.transformer import MultiheadAttention\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = MultiheadAttention(embed_dim, num_heads, **kwargs)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        # (batch, seq, embed_dim) => (seq, batch, embed_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        attn_mask = torch.full((x.size(0), x.size(0)), -float('inf'), device=x.device)\n",
        "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
        "\n",
        "        attn_output, _ = self.mha(query=x, key=x, value=x, attn_mask=attn_mask)\n",
        "        \n",
        "        # (seq, batch, embed_dim) => (batch, seq, embed_dim)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        \n",
        "        x = x.transpose(0, 1) # Reverse the earlier transpose operation\n",
        "        \n",
        "        x = x + attn_output\n",
        "        return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c66OTRwQfd8"
      },
      "source": [
        "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIY6Vu2pLBAO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.transformer import MultiheadAttention\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = MultiheadAttention(embed_dim, num_heads, **kwargs)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "  \n",
        "    def forward(self, x, y, **kwargs):\n",
        "        # (batch, seq, embed_dim) => (seq, batch, embed_dim)\n",
        "        x = x.transpose(0, 1)\n",
        "        y = y.transpose(0, 1)\n",
        "\n",
        "        attn_output, attention_scores = self.mha(query=x, key=y, value=y)\n",
        "\n",
        "        self.last_attention_scores = attention_scores\n",
        "        \n",
        "        # (seq, batch, embed_dim) => (batch, seq, embed_dim)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        \n",
        "        x = x.transpose(0, 1) # Reverse the earlier transpose operation\n",
        "        \n",
        "        x = x + attn_output\n",
        "        return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hn5p6f-RE0C"
      },
      "source": [
        "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWKrl7teOnH2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, units, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Linear(in_features=units, out_features=2*units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=2*units, out_features=units),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "        )\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(units)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = x + self.seq(x)\n",
        "        return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXoiVNPRoJc"
      },
      "source": [
        "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydcW5KZZHou7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = CausalSelfAttention(embed_dim=units, num_heads=num_heads, dropout=dropout_rate)\n",
        "        self.cross_attention = CrossAttention(embed_dim=units, num_heads=num_heads, dropout=dropout_rate)\n",
        "        self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        in_seq, out_seq = inputs\n",
        "\n",
        "        # Text input\n",
        "        out_seq = self.self_attention(out_seq)\n",
        "\n",
        "        out_seq = self.cross_attention(out_seq, in_seq)\n",
        "\n",
        "        self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "        out_seq = self.ff(out_seq)\n",
        "\n",
        "        return out_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgbYrF5Csqu"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnKZkrklAQf"
      },
      "source": [
        "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WQD87efena5"
      },
      "source": [
        "But there are a few other features you can add to make this work a little better:\n",
        "\n",
        "1. **Handle bad tokens**: The model will be generating text. It should\n",
        "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
        "   `'[START]'`). So set the bias for these to a large negative value.\n",
        "\n",
        "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
        "\n",
        "2. **Smart initialization**: The default initialization of a dense layer will\n",
        "  give a model that initially predicts each token with almost uniform\n",
        "  likelihood. The actual token distribution is far from uniform. The\n",
        "  optimal value for the initial bias of the output layer is the log of the\n",
        "  probability of each token. So include an `adapt` method to count the tokens\n",
        "  and set the optimal initial bias. This reduces the initial loss from the\n",
        "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
        "  entropy of the distribution (`-p*log(p)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeWw2SFDHUfo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TokenOutput(nn.Module):\n",
        "    def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dense = nn.Linear(in_features=tokenizer.vocab_size, out_features=tokenizer.vocab_size, **kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.banned_tokens = banned_tokens\n",
        "\n",
        "        self.bias = None\n",
        "\n",
        "    def adapt(self, ds):\n",
        "        counts = collections.Counter()\n",
        "        vocab_dict = {name: id\n",
        "                      for id, name in enumerate(self.tokenizer.get_vocab().keys())}\n",
        "\n",
        "        for tokens in tqdm(ds):\n",
        "            counts.update(tokens.numpy().flatten())\n",
        "\n",
        "        counts_arr = np.zeros(shape=(self.tokenizer.vocab_size,))\n",
        "        counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "        counts_arr = counts_arr[:]\n",
        "        for token in self.banned_tokens:\n",
        "            counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "        total = counts_arr.sum()\n",
        "        p = counts_arr/total\n",
        "        p[counts_arr==0] = 1.0\n",
        "        log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "        entropy = -(log_p*p).sum()\n",
        "\n",
        "        print()\n",
        "        print(f\"Uniform entropy: {np.log(self.tokenizer.vocab_size):0.2f}\")\n",
        "        print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "        self.bias = torch.tensor(log_p, dtype=torch.float32)\n",
        "        self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        return x + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQHqANd1A6Q"
      },
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGnOQyc501B2"
      },
      "outputs": [],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gq-ICN7bD-u"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou4fPH_SWgH"
      },
      "source": [
        "To build the model, you need to combine several parts:\n",
        "\n",
        "1. The image `feature_extractor` and the text `tokenizer` and.\n",
        "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
        "   vectors `(batch, sequence, channels)`.\n",
        "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
        "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHCISYehH1f6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Captioner(nn.Module):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "                 units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word_to_index = tokenizer.get_vocab()\n",
        "        self.index_to_word = {v: k for k, v in tokenizer.get_vocab().items()}\n",
        "\n",
        "        self.seq_embedding = SeqEmbedding(\n",
        "            vocab_size=len(tokenizer.get_vocab()),\n",
        "            depth=units,\n",
        "            max_length=max_length)\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "            for n in range(num_layers)])\n",
        "\n",
        "        self.output_layer = output_layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW390dOz9T-x"
      },
      "source": [
        "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
        "\n",
        "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
        "* If the text has dtype `tf.string` run it through the tokenizer.\n",
        "\n",
        "After that running the model is only a few steps:\n",
        "\n",
        "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
        "2. Look up the token embeddings.\n",
        "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
        "4. Run the output layer to predict the next token at each position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPdb7I4h9Ulo"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "\n",
        "@Captioner.add_method\n",
        "def forward(self, inputs):\n",
        "    image, txt = inputs\n",
        "\n",
        "    if image.shape[-1] == 3:\n",
        "        # Apply the feature-extractor, if you get an RGB image.\n",
        "        image = self.feature_extractor(image)\n",
        "\n",
        "    # Flatten the feature map\n",
        "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "    if isinstance(txt, list) and isinstance(txt[0], str):\n",
        "        # Apply the tokenizer if you get string inputs.\n",
        "        txt = self.tokenizer(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "        txt = dec_layer(inputs=(image, txt))\n",
        "\n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmM7aZQsLiyU"
      },
      "outputs": [],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "### Generate captions\n",
        "\n",
        "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
        "\n",
        "Start by downloading a test image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwFcdMqC-jE2"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBIiTkubmxA"
      },
      "source": [
        "To caption an image with this model:\n",
        "\n",
        "- Extract the `img_features`\n",
        "- Initialize the list of output tokens with a `[START]` token.\n",
        "- Pass `img_features` and `tokens` into the model.\n",
        "  - It returns a list of logits.\n",
        "  - Choose the next token based on those logits.  \n",
        "  - Add it to the list of tokens, and continue the loop.\n",
        "  - If it generates an `'[END]'` token, break out of the loop.\n",
        "\n",
        "So add a \"simple\" method to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf1Jie9ef_Cg"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN2NPX2zB8y"
      },
      "source": [
        "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPm96CccvHnq"
      },
      "outputs": [],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JefwCRZ8z-Ah"
      },
      "source": [
        "The temperature parameter allows you to interpolate between 3 modes:\n",
        "\n",
        "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
        "2. Random sampling according to the logits (`temperature=1.0`).\n",
        "3. Uniform random sampling (`temperature >> 1.0`). \n",
        "\n",
        "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0FpTvaPkqON"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcwZdqObK-U"
      },
      "source": [
        "To train the model you'll need several additional components:\n",
        "\n",
        "- The Loss and metrics\n",
        "- The Optimizer\n",
        "- Optional Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IW2mWa2sAG"
      },
      "source": [
        "### Losses and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbpbDQTw1lOW"
      },
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s24im3FqxAfT"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhjHqgv3F2e"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dyQN9UfJYEd"
      },
      "source": [
        "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKDwbZOCZ-AP"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNA3_RAsdl0"
      },
      "source": [
        "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGVLpzo13rcA"
      },
      "outputs": [],
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxp4KZRKDk9"
      },
      "source": [
        "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjzrwGZp23xx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaJhQpcG8u0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBXG0dCDKO55"
      },
      "source": [
        "Configure and execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OR5ZpAII__u"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro955bQ2KR0X"
      },
      "source": [
        "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
        "\n",
        "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aB0baOVMZe9"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P634LfVgw-eV"
      },
      "source": [
        "Plot the loss and accuracy over the training run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wn8KSkUw916"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZQ78b2Kxw-T"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQN1qT7KNqbL"
      },
      "source": [
        "## Attention plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XJaC2b2J23"
      },
      "source": [
        "Now, using the trained model,  run that `simple_gen` method on the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UQPtNTb2eu3"
      },
      "outputs": [],
      "source": [
        "result = model.simple_gen(image, temperature=0.0)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXbmeLGN1bJ"
      },
      "source": [
        "Split the output back into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKOpm0w5Xto"
      },
      "outputs": [],
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-AjuAV55Qo"
      },
      "source": [
        "The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpyuQvq2q-B"
      },
      "outputs": [],
      "source": [
        "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "[map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42ImsWv6oHG"
      },
      "source": [
        "So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojwtvnkh6mS-"
      },
      "outputs": [],
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence (height width) -> sequence height width',\n",
        "    height=7, width=7,\n",
        "    reduction='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TM7rA3zGpJW"
      },
      "source": [
        "Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWmWerGCZp3"
      },
      "outputs": [],
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7XYGFUd-U7"
      },
      "source": [
        "So here is where the model was focusing attention while generating each token of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "    \n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI4NAAws9rvY"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTz0abQKMkV"
      },
      "source": [
        "Now put that together into a more usable function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktpfW-SKQIJ"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "  \n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FntRkY11OiMw"
      },
      "outputs": [],
      "source": [
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "## Try it on your own images\n",
        "\n",
        "For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Psd1quzaAWg"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}